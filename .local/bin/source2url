#!/bin/bash
# Source2URL: This tool scans a source code directory and harvests URLs from it
# It then makes HTTP requests to each path via a configured proxy
# The purpose is to aid in content discovery during web assessments

# Check for command line arguments
if [ $# -ne 4 ]
then
 echo ""
 echo 'Syntax: Source2URL /some/dir root proxy url'
 echo 'Example: Source2URL ~/downloads/wordpress wordpress localhost:8080 domain.tld'
 echo "The root is the string that separates the parent directories from what we want."
 echo ""
 exit
fi

# Define command-line variables
DIR="$1"
ROOT="$2"
PROXY="$3"
URL="$4"

# Clear the temp file
echo "" > ./tempdirlist

# Configure the wget proxy
export http_proxy="$PROXY"

# Notifying on start of work
echo ""
echo "Initiating parsing..."

# Set delimiter for awk for directory cutoff
for i in `find $DIR -type f`
do
        echo $i | awk -F"$ROOT" '{ print $2 }' >> tempdirlist
done

# Prepend the base URL to the directories
sed -e "s/^/$URL/" tempdirlist > dirlist.txt

# Feedback
echo "Making queries..."

# Make the HTTP queries, quietly
for i in `cat dirlist.txt`
do
        wget --proxy $i  > /dev/null 2>&1
done

# Notify on completion.
echo "Process complete..."
